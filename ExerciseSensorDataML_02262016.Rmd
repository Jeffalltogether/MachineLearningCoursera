---
title: "Practicle Machine Learning Course Project"
author: "Jeffrey Thatcher"
date: "Monday, February 15, 2016"
output: html_document
---

You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.


```{r warning=F, message=F, echo=F}
### Set WD, Load Libraries and Data
setwd("C:/Users/jeffthatcher/Cloud Drive/RRepos/PracticalMachineLearning/PracticalMachineLearning")

library(caret)
library(ggplot2)
library(moments)
library(stringr)
library(dplyr)
library(tidyr)
source("multiplot.R")

###load training dataset
trainData <- as.data.frame(read.csv("pml-training.csv", na.strings=c("","NA")))

###generate ID variable
trainData$windowID <- paste(trainData$user_name, trainData$num_window,
                        trainData$classe,
                        sep=" ")

###Create unique identifier column for each user's activities
trainData$exerciseID <- paste(trainData$user_name,
                        trainData$classe, 
                        sep=" ")
###Graph timeseries
# generate times for each activity
trainData$time <- paste(str_sub(trainData$raw_timestamp_part_1, -5, -1),
                        substr(trainData$raw_timestamp_part_2, 1, 3), 
                        sep="")

trainData <- trainData %>%
        group_by(classe) %>%
        group_by(user_name) %>%
        mutate(newTime = as.numeric(time) - min(as.numeric(time)))
```

###Plot sensor data from one user
```{r warning=F, message=F, echo=F, fig.height=6, fig.width=6}
# plot timeseries for a few data points
p1 <- ggplot(trainData[grep("carlitos A", trainData$exerciseID),], aes(newTime, gyros_arm_z, color=user_name))+geom_line(color="green")

p2 <- ggplot(trainData[grep("carlitos B", trainData$exerciseID),], aes(newTime, gyros_arm_z, color=user_name))+geom_line(color="red")

p3 <- ggplot(trainData[grep("carlitos C", trainData$exerciseID),], aes(newTime, gyros_arm_z, color=user_name))+geom_line(color="red")

p4 <- ggplot(trainData[grep("carlitos D", trainData$exerciseID),], aes(newTime, gyros_arm_z, color=user_name))+geom_line(color="red")

p5 <- ggplot(trainData[grep("carlitos E", trainData$exerciseID),], aes(newTime, gyros_arm_z, color=user_name))+geom_line(color="red")

multiplot(p1, p2, p3, p4, p5, cols = 2)
```

###Calculate summary statistics for each value of the variable `num_window`
```{r warning=F, message=F, echo=F}
###Eliminate columns with missing and unrelated
cleanData <- function(dataframe){
 
        #Eliminate columns with unrelated data
        colToRemove <- c(grep("X", colnames(trainData)),
                         grep("user_name", colnames(trainData)),
                         grep("raw_timestamp_part_1", colnames(trainData)),
                         grep("raw_timestamp_part_2", colnames(trainData)),
                         grep("cvtd_timestamp", colnames(trainData)),
                         grep("new_window", colnames(trainData)),
                         grep("num_window", colnames(trainData)),
                         grep("time", colnames(trainData)),
                         grep("newTime", colnames(trainData)),
                         grep("classe", colnames(trainData)))
        
        dataframe <- dataframe[,-colToRemove]
        
        #Eliminate columns with NA values
        dataframe <- dataframe[, colSums(is.na(dataframe)) == 0]

        return(dataframe)
}
trainData <- cleanData(trainData)

###creat time series covariates/features
featureExtract <- function(dataframe, groupElement){
        
        temp.mean <- aggregate(dataframe, by = list(groupElement), FUN = "mean")
        temp.var <- aggregate(dataframe, by = list(groupElement), FUN = "var")
        temp.sd <- aggregate(dataframe, by = list(groupElement), FUN = "sd")
        temp.max <- aggregate(dataframe, by = list(groupElement), FUN = "max")
        temp.min <- aggregate(dataframe, by = list(groupElement), FUN = "min")
        temp.skew <- aggregate(dataframe, by = list(groupElement), FUN = "skewness")
        temp.kurt <- aggregate(dataframe, by = list(groupElement), FUN = "kurtosis")

        colnames(temp.mean) <- paste(colnames(temp.mean), "_mean", sep="")
        colnames(temp.var) <- paste(colnames(temp.var), "_var", sep="")
        colnames(temp.sd) <- paste(colnames(temp.sd), "_sd", sep="")
        colnames(temp.max) <- paste(colnames(temp.max), "_max", sep="")
        colnames(temp.min) <- paste(colnames(temp.min), "_min", sep="")
        colnames(temp.skew) <- paste(colnames(temp.skew), "_skew", sep="")
        colnames(temp.kurt) <- paste(colnames(temp.kurt), "_kurtosis", sep="")

        
        #join features into DF
        temp.features <- as.data.frame(cbind(temp.mean, temp.var, temp.sd, 
                                        temp.max, temp.min,
                                        temp.skew, temp.kurt))
        #remove columns with NA
        temp.features <- temp.features[, colSums(is.na(temp.features)) == 0]
        
        return(temp.features)
        }


featuresDF <- featureExtract(trainData[,2:51], trainData$windowID)

# Add `classe` and `user_name` variables back to feature DF
featuresDF$classe <- str_sub(featuresDF$Group.1_kurtosis, -1)
featuresDF$user_name <- str_sub(featuresDF$Group.1_kurtosis, 1, 5)

# remove extra group labels
featuresDF <- featuresDF[,-grep("Group", colnames(featuresDF))]
```


###Separate feature data into Training and Test Datasets
```{r warning=F, message=F, echo=F}
#Separate training and test data
# We will remove one person from each level of the variable `classe` as the test set
set.seed(1000)
trainIndex = createDataPartition(featuresDF$classe, p = 0.60,list=FALSE)
training = featuresDF[trainIndex,]
testing = featuresDF[-trainIndex,]
```



###Linear Discriminant Analysis
```{r warning=F, message=F, echo=F}
### Train lda Classifier
#fit model
Lda <- train(classe ~ ., method="lda", data=featuresDF)
        
#predict test data
pred <- predict(Lda, testing)
confusionMatrix(testing$classe, pred)
```


###Trees
```{r warning=F, message=F}
library(rattle)

#fit model
tree <- train(classe ~ ., method="rpart", data=featuresDF)

#predict test data
pred <- predict(tree, testing)
confusionMatrix(testing$classe, pred)

fancyRpartPlot(tree$finalModel)
```

###Bagging
```{r warning=F, message=F}
#fit model
bagging <- train(classe ~ ., method="treebag", data=featuresDF)

#predict test data
pred <- predict(bagging, testing)
confusionMatrix(testing$classe, pred)

fancyRpartPlot(summary(bagging)$mtrees[[25]]$btree)
```

