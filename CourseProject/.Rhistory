data(SAheart)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
colnames(trainSA)
LDA <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm", family = "binomial", data = trainSA)
LDA <- train(factor(chd) ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm", family = "binomial", data = trainSA)
head(trainSA$chd)
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
pred <- missClass(LDA, testSA)
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
pred <- missClass(LDA, testSA)
pred <- missClass(testSA, LDA)
class(testSA)
class(LDA)
LDA[[1]]
LDA[[2]]
LDA[[3]]
LDA[[4]]
LDA[[5]]
pred <- predict(LDA, testSA)
pred <- missClass(testSA$chd, pred)
pred
pred <- predict(LDA, testSA)
pred
LDA <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm", family = "binomial", data = trainSA)
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
pred <- predict(LDA, testSA)
pred <- missClass(testSA$chd, pred)
pred <- predict(LDA, trainSA)
missClass(testSA$chd, pred)
pred <- predict(LDA, testSA)
missClass(testSA$chd, pred)
pred <- predict(LDA, trainSA)
missClass(trainSA$chd, pred)
pred <- predict(LDA, testSA)
missClass(testSA$chd, pred)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
trainDF <- data.frame(vowel.train)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
trainDF <- data.frame(vowel.train)
testDF <- data.frame(vowel.test)
trainDF$y <- factor(trainDF$y)
testDF$y <- factor(testDF$y)
set.seed(33833)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
trainDF <- data.frame(vowel.train)
testDF <- data.frame(vowel.test)
trainDF$y <- factor(trainDF$y)
testDF$y <- factor(testDF$y)
set.seed(33833)
model <- train(y ~ ., method = "rf", data = trainDF)
varImp(model)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
trainDF <- data.frame(vowel.train)
testDF <- data.frame(vowel.test)
trainDF$y <- factor(trainDF$y)
testDF$y <- factor(testDF$y)
set.seed(33833)
model <- train(y ~ ., method = "rf", data = trainDF)
varImp(model)
DF <- rbind(trainDF, testDF)
set.seed(33833)
model <- train(y ~ ., method = "rf", data = DF)
varImp(model)
?varImp
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
trainDF <- data.frame(vowel.train)
testDF <- data.frame(vowel.test)
trainDF$y <- factor(trainDF$y)
testDF$y <- factor(testDF$y)
DF <- rbind(trainDF, testDF)
set.seed(33833)
model <- train(y ~ ., method = "rf", data = testDF)
varImp(model)
model <- train(y ~ ., method = "rf", data = trainDF)
varImp(model)
set.seed(33833)
model <- train(y ~ ., method = "rf", data = DF)
varImp(model)
varImp.RandomForest(model)
varImp.RandomForest(model)
order(varImp(model), decreasing = T)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
caret
?caret
??caret
model <- train(y ~ ., data = trainDF, method = "rf", prox=TRUE)
order(varImp(model), decreasing = T)
varImp(model)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
set.seed(33833)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
set.seed(33833)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
set.seed(33833)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
set.seed(33833)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
set.seed(33833)
model <- train(y ~ ., data = trainDF, method="rf")
order(varImp(model), decreasing = T)
pred <- predict(model, testDF)
confusionMatrix(testDF$y, pred)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
# Coronary Heart Disease (chd) as the outcome and age at onset, current alcohol
# consumption, obesity levels, cumulative tabacco, type-A behavior,
# and low density lipoprotein cholesterol as predictors
LDA <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm", family = "binomial", data = trainSA)
missClass = function(values,prediction){
sum(((prediction > 0.5)*1) != values)/length(values)
}
pred <- predict(LDA, trainSA)
missClass(trainSA$chd, pred)
pred <- predict(LDA, testSA)
missClass(testSA$chd, pred)
pred <- predict(LDA, trainSA)
train <- missClass(trainSA$chd, pred)
pred <- predict(LDA, testSA)
test <- missClass(testSA$chd, pred)
print("training missclassification", train, "testing missclassification", test)
print("training missclassification", train, "testing missclassification", test)
print(test, train)
pred <- predict(LDA, trainSA)
train <- missClass(trainSA$chd, pred)
pred <- predict(LDA, testSA)
test <- missClass(testSA$chd, pred)
test
train
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
trainDF <- data.frame(vowel.train)
testDF <- data.frame(vowel.test)
trainDF$y <- factor(trainDF$y)
testDF$y <- factor(testDF$y)
DF <- rbind(trainDF, testDF)
set.seed(33833)
model <- randomForest(y ~ ., data = trainDF)
order(varImp(model), decreasing = T)
## Research the prevalence of the disease in the population
prev = 0.5 #PAD prevalence for age 55+ with risk factors
## Chosing the parameters for the desired results of our test
# se = seq(0.7, 1.0, 0.05)
# sp = seq(0.7, 1.0, 0.05)
se = 0.86
sp = 0.86
desiredPPV = (prev*se)/(prev*se+(1-prev)*(1-sp))
desiredNPV= ((1-prev)*sp)/(prev*(1-se)+(1-prev)*sp)
## Determine NPV0 and PPV0
currentSE = 0.75   # sensitivity of current test to beat
currentSP = 0.75   # specificity of current test to beat
## Positive and negative predictive values
PPV0 = (prev*currentSE)/(prev*currentSE+(1-prev)*(1-currentSP))
NPV0 = ((1-prev)*currentSP)/(prev*(1-currentSE)+(1-prev)*currentSP)
## Calculate Sample Size
result <- nPV(se, sp, prev, NPV0, PPV0,
NPVpower = 0.8, PPVpower = 0.8,
rangeP = c(0.05, 0.95), nsteps = 30,
alpha = 0.05, setnames = NULL)
print(result)
# outDAT a data.frame showing the parameter settings (in rows) and the input parameters se, sp, prev, NPV0, PPV0, NPVpower, PPVpower, trueNPV, truePPV
# nlist a list with an element for each parameter setting in OUTDAT, listing the results of nNPV, and nPPV
# NSETS a single (integer), the number of parameter sets
# nsteps a single (integer), the number of steps in the range of proportions of true positives
# rangeP the input range of the proportion of true positives
# propP the resulting sequence of proportions of true positives considere
## Plot Result
plotnPV(result, log = "y", NPVpar = list(col=3, lwd=2, lty=1),
PPVpar = list(col=4, lwd=2, lty=1),
xlab="Proportion of true positives in study cohort, n1/(n0+n1) \n n0 = Shallow Burn     n1 = Deep Burn",
ylab="Total sample size, (n0+n1)")
abline(v=prev, col = "red")
#main="Figure Pilot Study \nSample Size Estimate")
library(bdpv)
## Research the prevalence of the disease in the population
prev = 0.5 #PAD prevalence for age 55+ with risk factors
## Chosing the parameters for the desired results of our test
# se = seq(0.7, 1.0, 0.05)
# sp = seq(0.7, 1.0, 0.05)
se = 0.86
sp = 0.86
desiredPPV = (prev*se)/(prev*se+(1-prev)*(1-sp))
desiredNPV= ((1-prev)*sp)/(prev*(1-se)+(1-prev)*sp)
## Determine NPV0 and PPV0
currentSE = 0.75   # sensitivity of current test to beat
currentSP = 0.75   # specificity of current test to beat
## Positive and negative predictive values
PPV0 = (prev*currentSE)/(prev*currentSE+(1-prev)*(1-currentSP))
NPV0 = ((1-prev)*currentSP)/(prev*(1-currentSE)+(1-prev)*currentSP)
## Calculate Sample Size
result <- nPV(se, sp, prev, NPV0, PPV0,
NPVpower = 0.8, PPVpower = 0.8,
rangeP = c(0.05, 0.95), nsteps = 30,
alpha = 0.05, setnames = NULL)
print(result)
# outDAT a data.frame showing the parameter settings (in rows) and the input parameters se, sp, prev, NPV0, PPV0, NPVpower, PPVpower, trueNPV, truePPV
# nlist a list with an element for each parameter setting in OUTDAT, listing the results of nNPV, and nPPV
# NSETS a single (integer), the number of parameter sets
# nsteps a single (integer), the number of steps in the range of proportions of true positives
# rangeP the input range of the proportion of true positives
# propP the resulting sequence of proportions of true positives considere
## Plot Result
plotnPV(result, log = "y", NPVpar = list(col=3, lwd=2, lty=1),
PPVpar = list(col=4, lwd=2, lty=1),
xlab="Proportion of true positives in study cohort, n1/(n0+n1) \n n0 = Shallow Burn     n1 = Deep Burn",
ylab="Total sample size, (n0+n1)")
abline(v=prev, col = "red")
#main="Figure Pilot Study \nSample Size Estimate")
str(nPV)
str(result)
result[[1]]
result[[2]]
result[[3]]
result[[4]]
result[[5]]
result[[7]]
result[[6]]
result[[2]]
prev = 0.2 #PAD prevalence for age 55+ with risk factors
## Chosing the parameters for the desired results of our test
# se = seq(0.7, 1.0, 0.05)
# sp = seq(0.7, 1.0, 0.05)
se = 0.86
sp = 0.86
desiredPPV = (prev*se)/(prev*se+(1-prev)*(1-sp))
desiredNPV= ((1-prev)*sp)/(prev*(1-se)+(1-prev)*sp)
## Research the prevalence of the disease in the population
prev = 0.5 #PAD prevalence for age 55+ with risk factors
## Chosing the parameters for the desired results of our test
# se = seq(0.7, 1.0, 0.05)
# sp = seq(0.7, 1.0, 0.05)
se = 0.86
sp = 0.86
desiredPPV = (prev*se)/(prev*se+(1-prev)*(1-sp))
desiredNPV= ((1-prev)*sp)/(prev*(1-se)+(1-prev)*sp)
## Determine NPV0 and PPV0
currentSE = 0.75   # sensitivity of current test to beat
currentSP = 0.75   # specificity of current test to beat
## Positive and negative predictive values
PPV0 = (prev*currentSE)/(prev*currentSE+(1-prev)*(1-currentSP))
NPV0 = ((1-prev)*currentSP)/(prev*(1-currentSE)+(1-prev)*currentSP)
## Calculate Sample Size
result <- nPV(se, sp, prev, NPV0, PPV0,
NPVpower = 0.8, PPVpower = 0.8,
rangeP = c(0.05, 0.95), nsteps = 30,
alpha = 0.05, setnames = NULL)
print(result)
# outDAT a data.frame showing the parameter settings (in rows) and the input parameters se, sp, prev, NPV0, PPV0, NPVpower, PPVpower, trueNPV, truePPV
# nlist a list with an element for each parameter setting in OUTDAT, listing the results of nNPV, and nPPV
# NSETS a single (integer), the number of parameter sets
# nsteps a single (integer), the number of steps in the range of proportions of true positives
# rangeP the input range of the proportion of true positives
# propP the resulting sequence of proportions of true positives considere
## Plot Result
plotnPV(result, log = "y", NPVpar = list(col=3, lwd=2, lty=1),
PPVpar = list(col=4, lwd=2, lty=1),
xlab="Proportion of true positives in study cohort, n1/(n0+n1) \n n0 = Shallow Burn     n1 = Deep Burn",
ylab="Total sample size, (n0+n1)")
abline(v=prev, col = "red")
#main="Figure Pilot Study \nSample Size Estimate")
samsize.mcnemar <- function(pi.01, pi.10, alpha, beta, sided)
{
pi.d <- (pi.01 + pi.10)
N <- (qnorm(1 - alpha/sided) * sqrt(pi.d) + qnorm(1 - beta) *
sqrt(pi.d - (pi.01 - pi.10)^2))^2/(pi.01 - pi.10)^2
return(ceiling(N))
}
# in my case, I am assuming that the 0 to 1 change will be 0.13 and that no
# one will change from 1 to 0
samsize.mcnemar(pi.01 = 0.13, pi.10 = 0, alpha = 0.1, beta = 0.2, sided = 2)
samsize.mcnemar(pi.01 = 0.11, pi.10 = 0, alpha = 0.1, beta = 0.2, sided = 2)
1-.86
samsize.mcnemar(pi.01 = 0.11, pi.10 = .2, alpha = 0.1, beta = 0.2, sided = 2)
library(caret)
predictors = data.frame(ozone=ozone$ozone)
library(MASS)
predictors = data.frame(ozone=ozone$ozone)
library(datasets)
predictors = data.frame(ozone=ozone$ozone)
??ozone
library(ELSR)
library(elsr)
library(ElemStatLearn)
predictors = data.frame(ozone=ozone$ozone)
?bag
?bagControl
?trcontrol
?trControl
??trcontrol
?qda
??Mass
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
QDAbag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = qda,
predict = predict.qda
aggregate = qda$aggregate)
library(caret)
library(ElemStatLearn)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
QDAbag <- bag(predictors, temperature, B = 10,
bagControl = bagControl(fit = qda,
predict = predict.qda,
aggregate = qda$aggregate))
qda$aggregate
### Set WD, Load Libraries and Data
setwd("C:/Users/jeffthatcher/Cloud Drive/RRepos/PracticalMachineLearning/MachineLearningCoursera/CourseProject")
library(caret)
library(ggplot2)
library(moments)
library(stringr)
library(dplyr)
library(tidyr)
source("multiplot.R")
###load training dataset
trainData <- as.data.frame(read.csv("pml-training.csv", na.strings=c("","NA")))
quizData <- as.data.frame(read.csv("pml-testing.csv", na.strings=c("","NA")))
###generate ID variable
trainData$windowID <- paste(trainData$user_name, trainData$num_window,
trainData$classe,
sep=" ")
###Create unique identifier column for each user's activities
trainData$exerciseID <- paste(trainData$user_name,
trainData$classe,
sep=" ")
###Graph timeseries
# generate times for each activity
trainData$time <- paste(str_sub(trainData$raw_timestamp_part_1, -5, -1),
substr(trainData$raw_timestamp_part_2, 1, 3),
sep="")
trainData <- trainData %>%
group_by(classe) %>%
group_by(user_name) %>%
mutate(newTime = as.numeric(time) - min(as.numeric(time)))
# plot timeseries for a few data points
p1 <- ggplot(trainData[grep("carlitos A", trainData$exerciseID),], aes(newTime, gyros_arm_y, color=user_name))+geom_line(color="green")
p2 <- ggplot(trainData[grep("carlitos B", trainData$exerciseID),], aes(newTime, gyros_arm_y, color=user_name))+geom_line(color="red")
p3 <- ggplot(trainData[grep("carlitos C", trainData$exerciseID),], aes(newTime, gyros_arm_y, color=user_name))+geom_line(color="red")
p4 <- ggplot(trainData[grep("carlitos D", trainData$exerciseID),], aes(newTime, gyros_arm_y, color=user_name))+geom_line(color="red")
p5 <- ggplot(trainData[grep("carlitos E", trainData$exerciseID),], aes(newTime, gyros_arm_y, color=user_name))+geom_line(color="red")
multiplot(p1, p2, p3, p4, p5, cols = 2)
###Eliminate columns with missing and unrelated
cleanData <- function(dataframe){
#Eliminate columns with unrelated data
colToRemove <- c(grep("X", colnames(dataframe)),
grep("user_name", colnames(dataframe)),
grep("raw_timestamp_part_1", colnames(dataframe)),
grep("raw_timestamp_part_2", colnames(dataframe)),
grep("cvtd_timestamp", colnames(dataframe)),
grep("new_window", colnames(dataframe)),
grep("num_window", colnames(dataframe)),
grep("time", colnames(dataframe)),
grep("newTime", colnames(dataframe)),
grep("exerciseID", colnames(dataframe)),
grep("windowID", colnames(dataframe)))
dataframe <- dataframe[,-colToRemove]
#Eliminate columns with NA values
dataframe <- dataframe[, colSums(is.na(dataframe)) == 0]
return(dataframe)
}
trainData <- cleanData(trainData)
quizData <- cleanData(quizData)
#Separate training and test data
# We will remove one person from each level of the variable `classe` as the test set
set.seed(1000)
trainIndex = createDataPartition(trainData$classe, p = 0.60,list=FALSE)
training = trainData[trainIndex,]
testing = trainData[-trainIndex,]
### Train lda Classifier
#fit model
Lda <- train(classe ~ ., method="lda", data=training)
#predict test data
pred <- predict(Lda, testing)
confusionMatrix(testing$classe, pred)
library(rattle)
#fit model
tree <- train(classe ~ ., method="rpart", data=training)
#predict test data
pred <- predict(tree, testing)
confusionMatrix(testing$classe, pred)
fancyRpartPlot(tree$finalModel)
### Set WD, Load Libraries and Data
setwd("C:/Users/jeffthatcher/Cloud Drive/RRepos/PracticalMachineLearning/MachineLearningCoursera/CourseProject")
library(caret)
library(ggplot2)
library(moments)
library(stringr)
library(dplyr)
library(tidyr)
source("multiplot.R")
###load training dataset
trainData <- as.data.frame(read.csv("pml-training.csv", na.strings=c("","NA")))
quizData <- as.data.frame(read.csv("pml-testing.csv", na.strings=c("","NA")))
###generate ID variable
trainData$windowID <- paste(trainData$user_name, trainData$num_window,
trainData$classe,
sep=" ")
###Create unique identifier column for each user's activities
trainData$exerciseID <- paste(trainData$user_name,
trainData$classe,
sep=" ")
###Graph timeseries
# generate times for each activity
trainData$time <- paste(str_sub(trainData$raw_timestamp_part_1, -5, -1),
substr(trainData$raw_timestamp_part_2, 1, 3),
sep="")
trainData <- trainData %>%
group_by(classe) %>%
group_by(user_name) %>%
mutate(newTime = as.numeric(time) - min(as.numeric(time)))
# plot timeseries for a few data points
p1 <- ggplot(trainData[grep("carlitos A", trainData$exerciseID),], aes(newTime, gyros_arm_y, color=user_name))+geom_line(color="green")
p2 <- ggplot(trainData[grep("carlitos B", trainData$exerciseID),], aes(newTime, gyros_arm_y, color=user_name))+geom_line(color="red")
p3 <- ggplot(trainData[grep("carlitos C", trainData$exerciseID),], aes(newTime, gyros_arm_y, color=user_name))+geom_line(color="red")
p4 <- ggplot(trainData[grep("carlitos D", trainData$exerciseID),], aes(newTime, gyros_arm_y, color=user_name))+geom_line(color="red")
p5 <- ggplot(trainData[grep("carlitos E", trainData$exerciseID),], aes(newTime, gyros_arm_y, color=user_name))+geom_line(color="red")
multiplot(p1, p2, p3, p4, p5, cols = 2)
###Eliminate columns with missing and unrelated
cleanData <- function(dataframe){
#Eliminate columns with unrelated data
colToRemove <- c(grep("X", colnames(dataframe)),
grep("user_name", colnames(dataframe)),
grep("raw_timestamp_part_1", colnames(dataframe)),
grep("raw_timestamp_part_2", colnames(dataframe)),
grep("cvtd_timestamp", colnames(dataframe)),
grep("new_window", colnames(dataframe)),
grep("num_window", colnames(dataframe)),
grep("time", colnames(dataframe)),
grep("newTime", colnames(dataframe)),
grep("exerciseID", colnames(dataframe)),
grep("windowID", colnames(dataframe)))
dataframe <- dataframe[,-colToRemove]
#Eliminate columns with NA values
dataframe <- dataframe[, colSums(is.na(dataframe)) == 0]
return(dataframe)
}
trainData <- cleanData(trainData)
quizData <- cleanData(quizData)
#Separate training and test data
# We will remove one person from each level of the variable `classe` as the test set
set.seed(1000)
trainIndex = createDataPartition(trainData$classe, p = 0.60,list=FALSE)
training = trainData[trainIndex,]
testing = trainData[-trainIndex,]
### Train lda Classifier
#fit model
Lda <- train(classe ~ ., method="lda", data=training)
#predict test data
pred <- predict(Lda, testing)
confusionMatrix(testing$classe, pred)
library(rattle)
#fit model
tree <- train(classe ~ ., method="rpart", data=training)
#predict test data
pred <- predict(tree, testing)
confusionMatrix(testing$classe, pred)
fancyRpartPlot(tree$finalModel)
